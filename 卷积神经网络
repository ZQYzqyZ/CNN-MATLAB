%% Convolution Neural Network Exercise

%  Instructions
%  ------------
% 
%  This file contains code that helps you get started in building a single.
%  layer convolutional nerual network. In this exercise, you will only
%  need to modify cnnCost.m and cnnminFuncSGD.m. You will not need to 
%  modify this file.


clear
close all
clc

%%======================================================================
%% STEP 0: Initialize Parameters and Load Data
%  Here we initialize some parameters used for the exercise.

% Configuration
imageDim = 28; %输入图像28*28
numClasses = 10;  % Number of classes (MNIST images fall into 10 classes)10个分类
filterDim1 = 5;    % Filter size for conv layer,
filterDim2 = 5;
numFilters1 = 20;   % Number of filters for conv layer
numFilters2 = 32;
poolDim1 = 2;      % Pooling dimension, (should divide imageDim-filterDim+1)池化维度
poolDim2 = 2; 

% Load MNIST Train
% addpath 'D:\我的文档\MATLAB\Deep Learning_exercise\data'; 
images = loadMNISTImages('E:\Program Files\MATLAB\Mat_work\Deep Learning_exercise\CNN\minist\train-images.idx3-ubyte');
images = reshape(images,imageDim,imageDim,[]);
labels = loadMNISTLabels('E:\Program Files\MATLAB\Mat_work\Deep Learning_exercise\CNN\minist\train-labels.idx1-ubyte');
labels(labels==0) = 10; % Remap 0 to 10

% Initialize
% Parameters,theta=(2165,1),2165=9*9*20+20+100*20*10+10,9*9*20是滤波器的权值，100*20*10是池化层与softmax连接的权值
theta = cnnInitParams(imageDim,filterDim1,filterDim2,numFilters1,numFilters2,poolDim1,poolDim2,numClasses);

%%======================================================================
%% STEP 1: Implement convNet Objective
%  Implement the function cnnCost.m.

%%======================================================================
%% STEP 2: Gradient Check
%  Use the file computeNumericalGradient.m to check the gradient
%  calculation for your cnnCost.m function.  You may need to add the
%  appropriate path or copy the file to this directory.

% DEBUG=false;  % set this to true to check gradient
DEBUG = true;
if DEBUG
    % To speed up gradient checking, we will use a reduced network and
    % a debugging data set
    db_numFilters1 = 2;
    db_numFilters2 = 4;
    db_filterDim1 = 5;
    db_filterDim2 = 5;
    db_poolDim1 = 2;
    db_poolDim2 = 2;
    db_images = images(:,:,1:10);
    db_labels = labels(1:10);
    db_theta = cnnInitParams(imageDim,db_filterDim1,db_filterDim2,db_numFilters1,db_numFilters2,db_poolDim1,db_poolDim2,numClasses);
    
    [cost grad] = cnnCost(db_theta,db_images,db_labels,numClasses,db_filterDim1,db_filterDim2,db_numFilters1,db_numFilters2,db_poolDim1,db_poolDim2);
    

    % Check gradients
    numGrad = computeNumericalGradient( @(x) cnnCost(x,db_images,...
                                db_labels,numClasses,db_filterDim1,db_filterDim2,...
                                db_numFilters1,db_numFilters2,db_poolDim1,db_poolDim2), db_theta);
 
    % Use this to visually compare the gradients side by side
    disp([numGrad grad]);
    
    diff = norm(numGrad-grad)/norm(numGrad+grad);
    % Should be small. In our implementation, these values are usually 
    % less than 1e-9.
    disp(diff); 
 
    assert(diff < 1e-9,...
        'Difference too large. Check your gradient computation again');
    
end;

%%======================================================================
%% STEP 3: Learn Parameters
%  Implement minFuncSGD.m, then train the model.

% 因为是采用的mini-batch梯度下降法，所以总共对样本的循环次数次数比标准梯度下降法要少
% 很多，因为每次循环中权值已经迭代多次了
options.epochs = 3; 
options.minibatch = 256;
options.alpha = 1e-1;
options.momentum = .95;

opttheta = minFuncSGD(@(x,y,z) cnnCost(x,y,z,numClasses,filterDim1,filterDim2,numFilters1,numFilters2,poolDim1,poolDim2),theta,images,labels,options);
save('theta.mat','opttheta');             

%%======================================================================
%% STEP 4: Test
%  Test the performance of the trained model using the MNIST test set. Your
%  accuracy should be above 97% after 3 epochs of training

testImages = loadMNISTImages('E:\Program Files\MATLAB\Mat_work\Deep Learning_exercise\CNN\minist\t10k-images.idx3-ubyte');
testImages = reshape(testImages,imageDim,imageDim,[]);
testLabels = loadMNISTLabels('E:\Program Files\MATLAB\Mat_work\Deep Learning_exercise\CNN\minist\t10k-labels.idx1-ubyte');
testLabels(testLabels==0) = 10; % Remap 0 to 10

[~,cost,preds]=cnnCost(opttheta,testImages,testLabels,numClasses,...
                filterDim,numFilters,poolDim,true);

acc = sum(preds==testLabels)/length(preds);

% Accuracy should be around 97.4% after 3 epochs
fprintf('Accuracy is %f\n',acc);
